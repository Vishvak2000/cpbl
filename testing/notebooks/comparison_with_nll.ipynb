{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 11:22:13.906988: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-12-09 11:22:13.907032: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "def multinomial_nll(true_counts, logits):\n",
    "    \"\"\"Compute the multinomial negative log-likelihood\n",
    "    Args:\n",
    "      true_counts: observed count values\n",
    "      logits: predicted logit values\n",
    "    \"\"\"\n",
    "    counts_per_example = tf.reduce_sum(true_counts, axis=-1)\n",
    "    print(tf.shape(counts_per_example))\n",
    "    dist = tfp.distributions.Multinomial(total_count=counts_per_example,\n",
    "                                         logits=logits)\n",
    "    log_probs = tf.reduce_sum(dist.log_prob(true_counts))\n",
    "    print(tf.shape(log_probs))\n",
    "    return (-tf.reduce_sum(dist.log_prob(true_counts)) / \n",
    "            tf.cast(tf.shape(true_counts)[0], dtype=tf.float32))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class MultinomialNLLLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits, true_counts):\n",
    "        # Compute log probabilities for each category\n",
    "        true_counts = torch.clamp(true_counts, min=1e-8)  # Avoid log(0)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        log_likelihood = torch.sum(true_counts * log_probs, dim=-1)\n",
    "        total_counts = true_counts.sum(dim=-1)\n",
    "        log_gamma_term = torch.lgamma(total_counts + 1)\n",
    "        log_gamma_counts = torch.lgamma(true_counts + 1).sum(dim=-1)\n",
    "        log_likelihood = log_likelihood + log_gamma_term - log_gamma_counts\n",
    "        return -torch.mean(log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join('../..')))\n",
    "\n",
    "from models._data import ChromatinDataset\n",
    "from models._model import CBPLTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chrs = [\"chr2\",\n",
    "    \"chr4\",\n",
    "    \"chr5\",\n",
    "    \"chr7\",\n",
    "    \"chr9\",\n",
    "    \"chr10\",\n",
    "    \"chr11\",\n",
    "    \"chr12\",\n",
    "    \"chr13\",\n",
    "    \"chr14\",\n",
    "    \"chr15\",\n",
    "    \"chr16\",\n",
    "    \"chr17\",\n",
    "    \"chr18\",\n",
    "    \"chr19\",\n",
    "    \"chr21\",\n",
    "    \"chr22\",\n",
    "    \"chrX\",\n",
    "    \"chrY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"peak_regions\": \"/gladstone/corces/lab/users/vishvak/chrombpnet_tutorial/pd_data/Microglia_peak_set_2.bed\",\n",
    "    \"nonpeak_regions\": \"/gladstone/corces/lab/users/vishvak/chrombpnet_tutorial/own_data/test.chr1.negatives.adjusted.bed\",\n",
    "    \"genome_fasta\": \"/gladstone/corces/lab/users/vishvak/chrombpnet_tutorial/data/downloads/hg38.fa\",\n",
    "    \"cts_bw_file\": \"/gladstone/corces/lab/users/vishvak/chrombpnet_tutorial/pd_data/nd_Microglia_merge.bw\",\n",
    "    \"negative_sampling_ratio\": 0,\n",
    "    \"train_size\": 0.6,\n",
    "    \"batch_size\": 32,\n",
    "    \"filters\": 512,\n",
    "    \"n_dil_layers\": 8,\n",
    "    \"conv1_kernel_size\": 21,\n",
    "    \"profile_kernel_size\": 71,\n",
    "    \"dilation_kernel_size\": 3,\n",
    "    \"input_seq_len\": 2114,\n",
    "    \"out_pred_len\": 1000,\n",
    "    \"dropout_rate\": 0.0,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"train_chrs\": train_chrs,\n",
    "    \"valid_chrs\": [\"chr1\"],\n",
    "    \"seq_focus_len\": 500,\n",
    "    \"use_cpu\": False,\n",
    "    \"alpha\" : 1,\n",
    "    \"checkpoint_path\": None,\n",
    "    \"flavor\" : None,\n",
    "    \"project\": \"cbpl_new_microglia\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in bed file of 151351 regions\n",
      "Read in bed file of 16900 regions\n",
      "Loaded 151351 peak regions and 0 non-peak regions\n",
      "Calculating average count for combined loss weight\n",
      "Average count per training peak = 135457.5\n",
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/wynton/home/corces/vishvak/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    }
   ],
   "source": [
    "trainer =  CBPLTrainer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in trainer.train_dataloader:\n",
    "    inputs, targets = batch\n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_nll = MultinomialNLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting size:torch.Size([32, 4, 2114])\n",
      "after first convolution:torch.Size([32, 512, 2094])\n",
      "after 1th dilation:torch.Size([32, 512, 2090])\n",
      "after 1th crop:torch.Size([32, 512, 2090])\n",
      "after 2th dilation:torch.Size([32, 512, 2082])\n",
      "after 2th crop:torch.Size([32, 512, 2082])\n",
      "after 3th dilation:torch.Size([32, 512, 2066])\n",
      "after 3th crop:torch.Size([32, 512, 2066])\n",
      "after 4th dilation:torch.Size([32, 512, 2034])\n",
      "after 4th crop:torch.Size([32, 512, 2034])\n",
      "after 5th dilation:torch.Size([32, 512, 1970])\n",
      "after 5th crop:torch.Size([32, 512, 1970])\n",
      "after 6th dilation:torch.Size([32, 512, 1842])\n",
      "after 6th crop:torch.Size([32, 512, 1842])\n",
      "after 7th dilation:torch.Size([32, 512, 1586])\n",
      "after 7th crop:torch.Size([32, 512, 1586])\n",
      "after 8th dilation:torch.Size([32, 512, 1074])\n",
      "after 8th crop:torch.Size([32, 512, 1074])\n",
      "Profile prediction shape: torch.Size([32, 1000])\n",
      "Count prediction shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "outputs = trainer.model.forward_test(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "#from https://github.com/kundajelab/basepair/blob/cda0875571066343cdf90aed031f7c51714d991a/basepair/losses.py#L87\n",
    "def multinomial_nll(true_counts, logits):\n",
    "    \"\"\"Compute the multinomial negative log-likelihood\n",
    "    Args:\n",
    "      true_counts: observed count values\n",
    "      logits: predicted logit values\n",
    "    \"\"\"\n",
    "    counts_per_example = tf.reduce_sum(true_counts, axis=-1)\n",
    "    dist = tfp.distributions.Multinomial(total_count=counts_per_example,\n",
    "                                         logits=logits)\n",
    "    return (-tf.reduce_sum(dist.log_prob(true_counts)) / \n",
    "            tf.cast(tf.shape(true_counts)[0], dtype=tf.float32))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 13:35:08.495966: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-12-03 13:35:08.496877: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2024-12-03 13:35:08.497680: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2024-12-03 13:35:08.498528: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2024-12-03 13:35:08.500478: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2024-12-03 13:35:08.502466: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2024-12-03 13:35:08.503245: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2024-12-03 13:35:08.503308: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-12-03 13:35:08.624366: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "tf_targets = tf.convert_to_tensor(targets[1].detach().numpy())\n",
    "tf_outputs = tf.convert_to_tensor(outputs[1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_input = tf.convert_to_tensor(inputs.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=44542.938>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_nll(tf_targets,tf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = tfp.distributions.Multinomial(total_count=tf.reduce_sum(tf_targets, axis=-1),\n",
    "                                         logits=tf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       "array([-206860.25  ,  -70000.375 ,   -9350.875 ,  -31095.25  ,\n",
       "       -105474.125 ,  -54320.875 ,  -84637.5   ,  -10018.797 ,\n",
       "        -18328.75  ,  -15417.531 ,  -45188.438 ,   -6810.375 ,\n",
       "        -20707.688 ,  -28331.375 , -191278.    ,  -35936.188 ,\n",
       "        -44314.    ,   -5278.3438, -262125.    ,  -74644.125 ,\n",
       "        -16878.125 ,  -22152.375 , -119300.    ,   -5027.5156,\n",
       "        -22823.438 ,   -9276.4375,  -22894.188 ,  -51361.    ,\n",
       "         -7402.9375,  -66321.31  ,  -44446.75  ,  -21035.5   ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.log_prob(tf_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.regression import PearsonCorrCoef, SpearmanCorrCoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr =PearsonCorrCoef(num_outputs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/wynton/home/corces/vishvak/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    }
   ],
   "source": [
    "spearmanr = SpearmanCorrCoef(num_outputs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0402,  0.0260,  0.0657,  0.0198, -0.0544, -0.0413,  0.0310,  0.0358,\n",
       "        -0.0217,  0.0934,  0.0363, -0.0154,  0.0559,  0.1400, -0.1085,  0.0459,\n",
       "        -0.0111, -0.0063, -0.0259,  0.0175,  0.0123,  0.0198, -0.0224, -0.0130,\n",
       "         0.0072,  0.0175, -0.0419, -0.0231,  0.0165, -0.1030, -0.0140, -0.0536])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(torch.from_numpy(tf_targets.numpy()).T,torch.from_numpy(tf_outputs.numpy()).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 1000), dtype=float32, numpy=\n",
       "array([[0.58572733, 0.8405155 , 0.8844119 , ..., 0.612815  , 0.6666953 ,\n",
       "        0.72798973],\n",
       "       [0.32442287, 0.65844226, 0.74438804, ..., 0.7804712 , 0.80494523,\n",
       "        0.7567023 ],\n",
       "       [0.6387386 , 0.6611079 , 0.6144929 , ..., 0.5243706 , 0.9197526 ,\n",
       "        0.6190738 ],\n",
       "       ...,\n",
       "       [0.7179155 , 0.83784884, 0.6963693 , ..., 0.77240825, 0.77436274,\n",
       "        0.8944525 ],\n",
       "       [0.7897969 , 0.7258214 , 0.7729998 , ..., 0.5469245 , 0.83745   ,\n",
       "        0.7259275 ],\n",
       "       [0.8847068 , 0.7565618 , 0.3998531 , ..., 0.5254934 , 0.8618141 ,\n",
       "        0.74292195]], dtype=float32)>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(54032.3711, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_nll(outputs[1],targets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "tf_model = tf.keras.models.load_model('/gladstone/corces/lab/users/vishvak/chrombpnet_tutorial/pd_data/microglia_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf_model.predict(tf_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=26408.703>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_nll(tf_targets,output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7434,  0.7201,  0.1712,  0.7233,  0.7153,  0.8335,  0.1701,  0.8567,\n",
       "         0.8875,  0.2959,  0.8642, -0.0159,  0.7315,  0.6346,  0.2625,  0.7861,\n",
       "         0.3647,  0.6048,  0.6864,  0.5203,  0.6476,  0.8209,  0.7014,  0.7393,\n",
       "         0.8539,  0.3961,  0.8314,  0.5989,  0.7010,  0.6842,  0.7242,  0.8672])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(torch.from_numpy(tf_targets.numpy()).T,torch.from_numpy(output[0]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7781, 0.6257, 0.1803, 0.6964, 0.7338, 0.7448, 0.1725, 0.8470, 0.8967,\n",
       "        0.2291, 0.8479, 0.0691, 0.6774, 0.6110, 0.1892, 0.7861, 0.4045, 0.6663,\n",
       "        0.8028, 0.5376, 0.4868, 0.8195, 0.7334, 0.7770, 0.7628, 0.3412, 0.8699,\n",
       "        0.6559, 0.6039, 0.7476, 0.5493, 0.8364])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr(torch.from_numpy(tf_targets.numpy()).T,torch.from_numpy(output[0]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 1000), dtype=float32, numpy=\n",
       "array([[ 45.,  46.,  48., ...,  75.,  75.,  76.],\n",
       "       [ 63.,  63.,  62., ...,   7.,   7.,   7.],\n",
       "       [ 17.,  17.,  17., ...,  12.,  12.,  12.],\n",
       "       ...,\n",
       "       [ 26.,  26.,  27., ...,  61.,  61.,  65.],\n",
       "       [ 27.,  23.,  23., ...,  96.,  97.,  97.],\n",
       "       [171., 182., 182., ...,  91.,  91.,  93.]], dtype=float32)>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.9208736],\n",
       "       [5.997568 ],\n",
       "       [7.559485 ],\n",
       "       [6.5625315],\n",
       "       [6.673019 ],\n",
       "       [7.1238184],\n",
       "       [6.8574533],\n",
       "       [6.983832 ],\n",
       "       [8.067585 ],\n",
       "       [7.1695156],\n",
       "       [7.6921005],\n",
       "       [6.895475 ],\n",
       "       [8.836632 ],\n",
       "       [6.688803 ],\n",
       "       [9.027556 ],\n",
       "       [7.5389924],\n",
       "       [8.572815 ],\n",
       "       [7.6638603],\n",
       "       [7.540666 ],\n",
       "       [6.5197425],\n",
       "       [6.483042 ],\n",
       "       [6.1656632],\n",
       "       [7.498359 ],\n",
       "       [8.099946 ],\n",
       "       [6.5437503],\n",
       "       [6.887114 ],\n",
       "       [7.012876 ],\n",
       "       [7.385822 ],\n",
       "       [6.803893 ],\n",
       "       [6.666781 ],\n",
       "       [6.840408 ],\n",
       "       [7.825834 ]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([  32, 2214,    4], dtype=int32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(tf_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chrombpnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
