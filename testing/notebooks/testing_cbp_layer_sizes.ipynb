{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, Cropping1D, add, Flatten, GlobalAvgPool1D, Dense\n",
    "\n",
    "# Define a minimal version of the model\n",
    "def recreate_model(sequence_len, out_pred_len, filters=64, n_dil_layers=4, conv1_kernel_size=21, profile_kernel_size=75, num_tasks=1):\n",
    "    # Input layer\n",
    "    inp = Input(shape=(sequence_len, 4), name='sequence')    \n",
    "\n",
    "    # First convolution\n",
    "    x = Conv1D(filters,\n",
    "               kernel_size=conv1_kernel_size,\n",
    "               padding='valid', \n",
    "               activation='relu',\n",
    "               name='conv1')(inp)\n",
    "\n",
    "    # Dilated convolutions with cropping\n",
    "    for i in range(1, n_dil_layers + 1):\n",
    "        conv_x = Conv1D(filters, \n",
    "                        kernel_size=3, \n",
    "                        padding='valid',\n",
    "                        activation='relu', \n",
    "                        dilation_rate=2**i,\n",
    "                        name=f'dilated_conv_{i}')(x)\n",
    "\n",
    "        x_len = x.shape[1]\n",
    "        conv_x_len = conv_x.shape[1]\n",
    "        crop_size = (x_len - conv_x_len) // 2\n",
    "        x = Cropping1D(crop_size, name=f'crop_{i}')(x)\n",
    "        x = add([conv_x, x])\n",
    "\n",
    "    # Profile prediction branch\n",
    "    prof_out_precrop = Conv1D(filters=num_tasks,\n",
    "                              kernel_size=profile_kernel_size,\n",
    "                              padding='valid',\n",
    "                              name='profile_conv')(x)\n",
    "\n",
    "    cropsize = int(prof_out_precrop.shape[1] / 2) - int(out_pred_len / 2)\n",
    "    prof = Cropping1D(cropsize, name='profile_crop')(prof_out_precrop)\n",
    "    profile_out = Flatten(name=\"profile_output\")(prof)\n",
    "\n",
    "    # Counts prediction branch\n",
    "    gap_combined_conv = GlobalAvgPool1D(name='gap')(x)\n",
    "    count_out = Dense(num_tasks, name=\"count_output\")(gap_combined_conv)\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=[inp], outputs=[profile_out, count_out])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 16:46:13.349757: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-11-18 16:46:13.350290: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2024-11-18 16:46:13.350835: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2024-11-18 16:46:13.351362: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2024-11-18 16:46:13.351823: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2024-11-18 16:46:13.352234: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2024-11-18 16:46:13.352670: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2024-11-18 16:46:13.353105: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2024-11-18 16:46:13.353120: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-11-18 16:46:13.354872: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 1000, 4)]    0           []                               \n",
      "                                                                                                  \n",
      " conv1 (Conv1D)                 (None, 980, 64)      5440        ['sequence[0][0]']               \n",
      "                                                                                                  \n",
      " dilated_conv_1 (Conv1D)        (None, 976, 64)      12352       ['conv1[0][0]']                  \n",
      "                                                                                                  \n",
      " crop_1 (Cropping1D)            (None, 976, 64)      0           ['conv1[0][0]']                  \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 976, 64)      0           ['dilated_conv_1[0][0]',         \n",
      "                                                                  'crop_1[0][0]']                 \n",
      "                                                                                                  \n",
      " dilated_conv_2 (Conv1D)        (None, 968, 64)      12352       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " crop_2 (Cropping1D)            (None, 968, 64)      0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 968, 64)      0           ['dilated_conv_2[0][0]',         \n",
      "                                                                  'crop_2[0][0]']                 \n",
      "                                                                                                  \n",
      " dilated_conv_3 (Conv1D)        (None, 952, 64)      12352       ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " crop_3 (Cropping1D)            (None, 952, 64)      0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 952, 64)      0           ['dilated_conv_3[0][0]',         \n",
      "                                                                  'crop_3[0][0]']                 \n",
      "                                                                                                  \n",
      " dilated_conv_4 (Conv1D)        (None, 920, 64)      12352       ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " crop_4 (Cropping1D)            (None, 920, 64)      0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 920, 64)      0           ['dilated_conv_4[0][0]',         \n",
      "                                                                  'crop_4[0][0]']                 \n",
      "                                                                                                  \n",
      " profile_conv (Conv1D)          (None, 846, 1)       4801        ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " profile_crop (Cropping1D)      (None, 500, 1)       0           ['profile_conv[0][0]']           \n",
      "                                                                                                  \n",
      " gap (GlobalAveragePooling1D)   (None, 64)           0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " profile_output (Flatten)       (None, 500)          0           ['profile_crop[0][0]']           \n",
      "                                                                                                  \n",
      " count_output (Dense)           (None, 1)            65          ['gap[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 59,714\n",
      "Trainable params: 59,714\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Input shape: (1, 1000, 4)\n",
      "Profile output shape: (1, 500)\n",
      "Count output shape: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example parameters\n",
    "sequence_len = 1000\n",
    "out_pred_len = 500\n",
    "filters = 64\n",
    "n_dil_layers = 4\n",
    "\n",
    "# Create and summarize the model\n",
    "model = recreate_model(sequence_len, out_pred_len, filters=filters, n_dil_layers=n_dil_layers)\n",
    "model.summary()\n",
    "\n",
    "# Dummy data to check shapes\n",
    "import numpy as np\n",
    "dummy_input = np.random.random((1, sequence_len, 4)).astype(np.float32)\n",
    "profile_out, count_out = model.predict(dummy_input)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Input shape:\", dummy_input.shape)\n",
    "print(\"Profile output shape:\", profile_out.shape)\n",
    "print(\"Count output shape:\", count_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CNN output shape: torch.Size([8, 1, 998])\n",
      "PyTorch Flatten output shape: torch.Size([8, 497])\n",
      "PyTorch Crop output shape: torch.Size([8, 1, 497])\n",
      "TensorFlow CNN output shape: (8, 998, 1)\n",
      "TensorFlow Flatten output shape: (8, 495)\n",
      "TensorFlow Crop output shape: (8, 495, 1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch CNN module and Flatten module\n",
    "class CNNModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, activation_fn=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=in_channels,\n",
    "                            out_channels=out_channels, \n",
    "                            kernel_size=kernel_size, \n",
    "                            stride=stride, \n",
    "                            padding=padding)\n",
    "        self.activation = activation_fn()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return x\n",
    "    \n",
    "class Cropping1D(nn.Module):\n",
    "    def __init__(self, crop_size):\n",
    "        super().__init__()\n",
    "        self.crop_size = crop_size  # Total amount to crop (both sides combined)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # For skip connections, we need to crop from both ends equally\n",
    "        crop_left = self.crop_size // 2\n",
    "        crop_right = self.crop_size - crop_left\n",
    "        \n",
    "        if crop_right > 0:\n",
    "            return x[:, :, crop_left:-crop_right]\n",
    "        return x[:, :, crop_left:]\n",
    "\n",
    "batch_size = 8\n",
    "channels = 4\n",
    "seq_length = 1000\n",
    "\n",
    "output_length=500\n",
    "input_tensor = torch.randn(batch_size, channels, seq_length)  # (batch_size, channels, sequence_length)\n",
    "\n",
    "# PyTorch model pipeline\n",
    "cnn_module = CNNModule(in_channels=channels, out_channels=1, kernel_size=3, stride=1, padding=0)\n",
    "flatten_module = Flatten()\n",
    "\n",
    "cropping_module = Cropping1D(1+(seq_length-output_length))\n",
    "\n",
    "# Passing through CNN and Flatten in PyTorch\n",
    "cnn_output = cnn_module(input_tensor)\n",
    "cnn_output_shape = cnn_output.shape\n",
    "cropping_output = cropping_module(cnn_output)\n",
    "cropping_output_shape = cropping_output.shape\n",
    "flatten_output = flatten_module(cropping_output)\n",
    "flatten_output_shape = flatten_output.shape\n",
    "\n",
    "\n",
    "# TensorFlow model equivalent\n",
    "input_tensor_tf = tf.random.normal([batch_size, seq_length, channels])  # (batch_size, sequence_length, channels)\n",
    "\n",
    "# TensorFlow CNN layer\n",
    "conv_layer_tf = tf.keras.layers.Conv1D(filters=1, kernel_size=3, strides=1, padding='valid', activation='relu')\n",
    "conv_output_tf = conv_layer_tf(input_tensor_tf)\n",
    "conv_output_shape_tf = conv_output_tf.shape\n",
    "\n",
    "# TensorFlow Cropping layer (Cropping1D equivalent)\n",
    "crop_layer_tf = tf.keras.layers.Cropping1D(cropping=(1 + (seq_length - output_length) // 2 , 1 +(seq_length - output_length) // 2 +1))\n",
    "crop_output_tf = crop_layer_tf(conv_output_tf)\n",
    "crop_output_tf_shape = crop_output_tf.shape\n",
    "\n",
    "# TensorFlow Flatten layer\n",
    "flatten_layer_tf = tf.keras.layers.Flatten()\n",
    "flatten_output_tf = flatten_layer_tf(crop_output_tf)\n",
    "flatten_output_shape_tf = flatten_output_tf.shape\n",
    "\n",
    "# Print shapes for comparison\n",
    "print(f\"PyTorch CNN output shape: {cnn_output_shape}\")\n",
    "print(f\"PyTorch Flatten output shape: {flatten_output_shape}\")\n",
    "print(f\"PyTorch Crop output shape: {cropping_output_shape}\")\n",
    "\n",
    "print(f\"TensorFlow CNN output shape: {conv_output_shape_tf}\")\n",
    "print(f\"TensorFlow Flatten output shape: {flatten_output_shape_tf}\")\n",
    "print(f\"TensorFlow Crop output shape: {crop_output_tf_shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MultinomialNLLLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Multinomial Negative Log-Likelihood Loss.\n",
    "        \n",
    "        Computes the negative log-likelihood for multinomial distribution.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, logits, true_counts):\n",
    "        \"\"\"\n",
    "        Compute the multinomial negative log-likelihood.\n",
    "        \n",
    "        Args:\n",
    "            logits (torch.Tensor): Predicted logit values \n",
    "                Shape: (batch_size, num_categories)\n",
    "            true_counts (torch.Tensor): Observed count values \n",
    "                Shape: (batch_size, num_categories)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Average negative log-likelihood across the batch\n",
    "        \"\"\"\n",
    "        # Compute log probabilities for each category\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # Compute total counts per example\n",
    "        total_counts = true_counts.sum(dim=-1)\n",
    "        \n",
    "        # Compute the log-likelihood manually\n",
    "        # This is equivalent to the multinomial distribution log probability\n",
    "        likelihood = torch.sum(true_counts * log_probs, dim=-1)\n",
    "        \n",
    "        # Apply constraint that probabilities sum to 1 and match total count\n",
    "        # This is similar to the multinomial distribution constraint\n",
    "        log_likelihood = likelihood - torch.lgamma(total_counts + 1)\n",
    "        \n",
    "        # Return negative mean log-likelihood\n",
    "        return -log_likelihood.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss\n",
    "loss_fn = MultinomialNLLLoss()\n",
    "\n",
    "# Assume you have your logits and true counts\n",
    "logits = torch.randn(32, 1000)  # 32 examples, 10 categories\n",
    "true_counts = torch.randint(0, 10, (32, 1000)).float()\n",
    "\n",
    "# Compute loss\n",
    "loss = loss_fn(logits, true_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66587.3594)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple, List\n",
    "\n",
    "class ChromBPNetTest(nn.Module):\n",
    "    def __init__(self, \n",
    "                sequence_len: int = 1000,\n",
    "                out_pred_len: int = 1000,\n",
    "                filters: int = 64,\n",
    "                n_dil_layers: int = 8,\n",
    "                conv1_kernel_size: int = 21,\n",
    "                profile_kernel_size: int = 75):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(\"\\nInitializing ChromBPNet Test Network\")\n",
    "        print(f\"Input sequence length: {sequence_len}\")\n",
    "        print(f\"Target output length: {out_pred_len}\")\n",
    "        print(f\"Number of filters: {filters}\")\n",
    "        print(f\"Number of dilated layers: {n_dil_layers}\")\n",
    "        print(f\"First conv kernel size: {conv1_kernel_size}\")\n",
    "        print(f\"Profile conv kernel size: {profile_kernel_size}\\n\")\n",
    "        \n",
    "        # Track shapes at each layer\n",
    "        self.layer_shapes = {}\n",
    "        \n",
    "        # Initial conv with no dilation\n",
    "        self.initial_conv = nn.Conv1d(4, filters, kernel_size=conv1_kernel_size, padding=0)\n",
    "        \n",
    "        # Dilated convolution layers\n",
    "        self.dilated_convs = nn.ModuleList()\n",
    "        for i in range(1, n_dil_layers + 1):\n",
    "            dilation = 2 ** i\n",
    "            self.dilated_convs.append(\n",
    "                nn.Conv1d(filters, filters, kernel_size=3, \n",
    "                         padding=0, dilation=dilation)\n",
    "            )\n",
    "        \n",
    "        # Profile prediction branch\n",
    "        self.profile_conv = nn.Conv1d(filters, 1, kernel_size=profile_kernel_size, padding=0)\n",
    "        \n",
    "        # Count prediction branch\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.count_dense = nn.Linear(filters, 1)\n",
    "        \n",
    "    def calculate_conv1d_output_length(self, length: int, kernel_size: int, \n",
    "                                     stride: int = 1, padding: int = 0, \n",
    "                                     dilation: int = 1) -> int:\n",
    "        \"\"\"Calculate output length for 1D convolution\"\"\"\n",
    "        return int((length + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, verbose: bool = True) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass with shape printing\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, 4, sequence_length)\n",
    "            verbose: Whether to print shape information\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"\\nInput shape: {x.shape}\")\n",
    "        self.layer_shapes['input'] = x.shape\n",
    "        \n",
    "        # Initial convolution\n",
    "        x = self.initial_conv(x)\n",
    "        if verbose:\n",
    "            print(f\"After initial conv: {x.shape}\")\n",
    "        self.layer_shapes['initial_conv'] = x.shape\n",
    "        \n",
    "        # Store original shape for residual connections\n",
    "        original_x = x\n",
    "        \n",
    "        # Dilated convolutions with residual connections\n",
    "        for i, conv in enumerate(self.dilated_convs, 1):\n",
    "            # Apply dilated convolution\n",
    "            conv_x = conv(x)\n",
    "            \n",
    "            # Calculate cropping for residual\n",
    "            crop_size = (x.shape[-1] - conv_x.shape[-1]) // 2\n",
    "            x_cropped = x[:, :, crop_size:-crop_size]\n",
    "            \n",
    "            # Add residual connection\n",
    "            x = conv_x + x_cropped\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"After dilated conv {i} (dilation={2**i}): {x.shape}\")\n",
    "            self.layer_shapes[f'dilated_conv_{i}'] = x.shape\n",
    "        \n",
    "        # Profile prediction branch\n",
    "        profile_out = self.profile_conv(x)\n",
    "        if verbose:\n",
    "            print(f\"After profile conv: {profile_out.shape}\")\n",
    "        self.layer_shapes['profile_conv'] = profile_out.shape\n",
    "        \n",
    "        # Flatten profile output\n",
    "        profile_out = profile_out.squeeze(1)  # Remove channel dimension\n",
    "        if verbose:\n",
    "            print(f\"Profile output shape: {profile_out.shape}\")\n",
    "        self.layer_shapes['profile_output'] = profile_out.shape\n",
    "        \n",
    "        # Count prediction branch\n",
    "        count_x = self.global_avg_pool(x)\n",
    "        count_x = count_x.squeeze(-1)  # Remove spatial dimension\n",
    "        count_out = self.count_dense(count_x)\n",
    "        if verbose:\n",
    "            print(f\"Count output shape: {count_out.shape}\")\n",
    "        self.layer_shapes['count_output'] = count_out.shape\n",
    "        \n",
    "        return count_out, profile_out\n",
    "    \n",
    "    def print_layer_summary(self):\n",
    "        \"\"\"Print summary of all layer shapes\"\"\"\n",
    "        print(\"\\nLayer Shape Summary:\")\n",
    "        print(\"-\" * 50)\n",
    "        for layer_name, shape in self.layer_shapes.items():\n",
    "            print(f\"{layer_name:.<30} {str(shape)}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Test the network\n",
    "def test_network():\n",
    "    # Set parameters\n",
    "    batch_size = 4\n",
    "    sequence_len = 1000\n",
    "    out_pred_len = 1000\n",
    "    \n",
    "    # Create model\n",
    "    model = ChromBPNetTest(\n",
    "        sequence_len=sequence_len,\n",
    "        out_pred_len=out_pred_len,\n",
    "        filters=64,\n",
    "        n_dil_layers=8,\n",
    "        conv1_kernel_size=21,\n",
    "        profile_kernel_size=75\n",
    "    )\n",
    "    \n",
    "    # Create dummy input\n",
    "    x = torch.randn(batch_size, 4, sequence_len)\n",
    "    \n",
    "    # Forward pass\n",
    "    print(\"\\nRunning forward pass...\")\n",
    "    count_out, profile_out = model(x)\n",
    "    \n",
    "    # Print layer summary\n",
    "    model.print_layer_summary()\n",
    "    \n",
    "    print(\"\\nNetwork output sizes:\")\n",
    "    print(f\"Count output: {count_out.shape}\")\n",
    "    print(f\"Profile output: {profile_out.shape}\")\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.framework.type_spec' has no attribute '_NAME_TO_TYPE_SPEC'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/wynton/home/corces/vishvak/pytorch_cbp/testing/notebooks/testing_cbp_layer_sizes.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpudev1.wynton.ucsf.edu/wynton/home/corces/vishvak/pytorch_cbp/testing/notebooks/testing_cbp_layer_sizes.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpudev1.wynton.ucsf.edu/wynton/home/corces/vishvak/pytorch_cbp/testing/notebooks/testing_cbp_layer_sizes.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpudev1.wynton.ucsf.edu/wynton/home/corces/vishvak/pytorch_cbp/testing/notebooks/testing_cbp_layer_sizes.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtfp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpudev1.wynton.ucsf.edu/wynton/home/corces/vishvak/pytorch_cbp/testing/notebooks/testing_cbp_layer_sizes.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpudev1.wynton.ucsf.edu/wynton/home/corces/vishvak/pytorch_cbp/testing/notebooks/testing_cbp_layer_sizes.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmultinomial_nll\u001b[39m(true_counts, logits):\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/__init__.py:23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m \u001b[39mimport\u001b[39;00m substrates\n\u001b[1;32m     21\u001b[0m \u001b[39m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m# from tensorflow_probability.google import tfp_google  # DisableOnExport\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/__init__.py:138\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m _tf_loaded():\n\u001b[1;32m    136\u001b[0m   \u001b[39m# Non-lazy load of packages that register with tensorflow or keras.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m   \u001b[39mfor\u001b[39;00m pkg_name \u001b[39min\u001b[39;00m _maybe_nonlazy_load:\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mdir\u001b[39;49m(\u001b[39mglobals\u001b[39;49m()[pkg_name])  \u001b[39m# Forces loading the package from its lazy loader.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m all_util\u001b[39m.\u001b[39mremove_undocumented(\u001b[39m__name__\u001b[39m, _lazy_load \u001b[39m+\u001b[39m _maybe_nonlazy_load)\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/internal/lazy_loader.py:57\u001b[0m, in \u001b[0;36mLazyLoader.__dir__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__dir__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 57\u001b[0m   module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load()\n\u001b[1;32m     58\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mdir\u001b[39m(module)\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/internal/lazy_loader.py:40\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_first_access \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_module_globals \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_module_globals[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_name] \u001b[39m=\u001b[39m module\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/experimental/__init__.py:31\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"TensorFlow Probability API-unstable package.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39mThis package contains potentially useful code which is under active development\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mYou are welcome to try any of this out (and tell us how well it works for you!).\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m auto_batching\n\u001b[0;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m bijectors\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m distributions\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/experimental/bijectors/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2020 The TensorFlow Probability Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# ============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"TensorFlow Probability experimental bijectors package.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbijectors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mldj_ratio\u001b[39;00m \u001b[39mimport\u001b[39;00m forward_log_det_jacobian_ratio\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbijectors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mldj_ratio\u001b[39;00m \u001b[39mimport\u001b[39;00m inverse_log_det_jacobian_ratio\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbijectors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribution_bijectors\u001b[39;00m \u001b[39mimport\u001b[39;00m make_distribution_bijector\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/bijectors/__init__.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Bijective transformations.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# pylint: disable=unused-import,wildcard-import,line-too-long,g-importing-member\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbijectors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mabsolute_value\u001b[39;00m \u001b[39mimport\u001b[39;00m AbsoluteValue\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbijectors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mascending\u001b[39;00m \u001b[39mimport\u001b[39;00m Ascending\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbijectors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatch_normalization\u001b[39;00m \u001b[39mimport\u001b[39;00m BatchNormalization\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/bijectors/absolute_value.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"AbsoluteValue bijector.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbijectors\u001b[39;00m \u001b[39mimport\u001b[39;00m bijector\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m assert_util\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m dtype_util\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/bijectors/bijector.py:34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m slicing\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m tensorshape_util\n\u001b[0;32m---> 34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m \u001b[39mimport\u001b[39;00m generic \u001b[39mas\u001b[39;00m math_generic\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m \u001b[39mimport\u001b[39;00m gradient\n\u001b[1;32m     37\u001b[0m \u001b[39m# pylint: disable=g-direct-tensorflow-import\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/math/__init__.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m all_util\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m \u001b[39mimport\u001b[39;00m ode\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m \u001b[39mimport\u001b[39;00m psd_kernels\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbessel\u001b[39;00m \u001b[39mimport\u001b[39;00m bessel_iv_ratio\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbessel\u001b[39;00m \u001b[39mimport\u001b[39;00m bessel_ive\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/math/psd_kernels/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Positive-semidefinite kernels package.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m all_util\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpsd_kernels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchangepoint\u001b[39;00m \u001b[39mimport\u001b[39;00m ChangePoint\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpsd_kernels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexp_sin_squared\u001b[39;00m \u001b[39mimport\u001b[39;00m ExpSinSquared\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpsd_kernels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexponentiated_quadratic\u001b[39;00m \u001b[39mimport\u001b[39;00m ExponentiatedQuadratic\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/math/psd_kernels/changepoint.py:28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m prefer_static \u001b[39mas\u001b[39;00m ps\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_util\n\u001b[0;32m---> 28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpsd_kernels\u001b[39;00m \u001b[39mimport\u001b[39;00m positive_semidefinite_kernel \u001b[39mas\u001b[39;00m psd_kernel\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpsd_kernels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m util\n\u001b[1;32m     31\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mChangePoint\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/math/psd_kernels/positive_semidefinite_kernel.py:1129\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(_AutoCompositeTensorPsdKernelMeta, mcs)\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(  \u001b[39m# pylint: disable=too-many-function-args\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m         mcs, classname, baseclasses, attrs)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mreturn\u001b[39;00m auto_composite_tensor\u001b[39m.\u001b[39mauto_composite_tensor(\n\u001b[1;32m   1123\u001b[0m         \u001b[39mcls\u001b[39m,\n\u001b[1;32m   1124\u001b[0m         omit_kwargs\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mparameters\u001b[39m\u001b[39m'\u001b[39m,),\n\u001b[1;32m   1125\u001b[0m         non_identifying_kwargs\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m,),\n\u001b[1;32m   1126\u001b[0m         module_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtfp.math._psdkernels\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1129\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAutoCompositeTensorPsdKernel\u001b[39;00m(PositiveSemidefiniteKernel,\n\u001b[1;32m   1130\u001b[0m                                    auto_composite_tensor\u001b[39m.\u001b[39mAutoCompositeTensor,\n\u001b[1;32m   1131\u001b[0m                                    metaclass\u001b[39m=\u001b[39m_AutoCompositeTensorPsdKernelMeta):\n\u001b[1;32m   1132\u001b[0m   \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_flatten_summand_list\u001b[39m(kernels):\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/math/psd_kernels/positive_semidefinite_kernel.py:1122\u001b[0m, in \u001b[0;36m_AutoCompositeTensorPsdKernelMeta.__new__\u001b[0;34m(mcs, classname, baseclasses, attrs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Give subclasses their own type_spec, not an inherited one.\"\"\"\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(_AutoCompositeTensorPsdKernelMeta, mcs)\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(  \u001b[39m# pylint: disable=too-many-function-args\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     mcs, classname, baseclasses, attrs)\n\u001b[0;32m-> 1122\u001b[0m \u001b[39mreturn\u001b[39;00m auto_composite_tensor\u001b[39m.\u001b[39;49mauto_composite_tensor(\n\u001b[1;32m   1123\u001b[0m     \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m   1124\u001b[0m     omit_kwargs\u001b[39m=\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mparameters\u001b[39;49m\u001b[39m'\u001b[39;49m,),\n\u001b[1;32m   1125\u001b[0m     non_identifying_kwargs\u001b[39m=\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m'\u001b[39;49m,),\n\u001b[1;32m   1126\u001b[0m     module_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtfp.math._psdkernels\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/internal/auto_composite_tensor.py:598\u001b[0m, in \u001b[0;36mauto_composite_tensor\u001b[0;34m(cls, omit_kwargs, non_identifying_kwargs, module_name)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[39m# If the declared class is already a CompositeTensor subclass, we can avoid\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[39m# affecting the actual type of the returned class. Otherwise, we need to\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[39m# explicitly mix in the CT type, and hence create and return a newly\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[39m# synthesized type.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, composite_tensor\u001b[39m.\u001b[39mCompositeTensor):\n\u001b[0;32m--> 598\u001b[0m   \u001b[39m@type_spec_register\u001b[39m(type_spec_name)\n\u001b[1;32m    599\u001b[0m   \u001b[39mclass\u001b[39;00m \u001b[39m_AlreadyCTTypeSpec\u001b[39;00m(_AutoCompositeTensorTypeSpec):\n\u001b[1;32m    601\u001b[0m     \u001b[39m@property\u001b[39m\n\u001b[1;32m    602\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mvalue_type\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    603\u001b[0m       \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/internal/auto_composite_tensor.py:441\u001b[0m, in \u001b[0;36mtype_spec_register\u001b[0;34m(name, allow_overwrite)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Decorator used to register a unique name for a TypeSpec subclass.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \n\u001b[1;32m    424\u001b[0m \u001b[39mUnlike TensorFlow's `type_spec.register`, this function allows a new\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39m  A class decorator that registers the decorated class with the given name.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m \u001b[39mif\u001b[39;00m allow_overwrite \u001b[39mand\u001b[39;00m name \u001b[39min\u001b[39;00m type_spec\u001b[39m.\u001b[39;49m_NAME_TO_TYPE_SPEC:\n\u001b[1;32m    442\u001b[0m   type_spec\u001b[39m.\u001b[39m_TYPE_SPEC_TO_NAME\u001b[39m.\u001b[39mpop(\n\u001b[1;32m    443\u001b[0m       type_spec\u001b[39m.\u001b[39m_NAME_TO_TYPE_SPEC\u001b[39m.\u001b[39mpop(name))\n\u001b[1;32m    444\u001b[0m \u001b[39mreturn\u001b[39;00m type_spec\u001b[39m.\u001b[39mregister(name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.framework.type_spec' has no attribute '_NAME_TO_TYPE_SPEC'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "def multinomial_nll(true_counts, logits):\n",
    "    counts_per_example = tf.reduce_sum(true_counts, axis=-1)\n",
    "    dist = tfp.distributions.Multinomial(total_count=counts_per_example, logits=logits)\n",
    "    return (-tf.reduce_sum(dist.log_prob(true_counts)) / tf.cast(tf.shape(true_counts)[0], dtype=tf.float32))\n",
    "\n",
    "# PyTorch Implementation\n",
    "class MultinomialNLLLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits, true_counts):\n",
    "        # Compute log probabilities for each category\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        print(\"PyTorch Log Probabilities:\")\n",
    "        print(log_probs)\n",
    "\n",
    "        # Compute the log-likelihood manually\n",
    "        likelihood = torch.sum(true_counts * log_probs, dim=-1)\n",
    "        print(\"PyTorch Likelihoods:\")\n",
    "        print(likelihood)\n",
    "\n",
    "        # Return negative mean log-likelihood\n",
    "        return -likelihood.mean()\n",
    "\n",
    "# Generate random input data\n",
    "batch_size = 32\n",
    "num_categories = 1000\n",
    "\n",
    "# Random logits and true counts for testing\n",
    "np.random.seed(42)\n",
    "logits_np = np.random.randn(batch_size, num_categories).astype(np.float32)\n",
    "true_counts_np = np.random.randint(1, 10, size=(batch_size, num_categories)).astype(np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "logits_pt = torch.tensor(logits_np, requires_grad=True)\n",
    "true_counts_pt = torch.tensor(true_counts_np)\n",
    "\n",
    "# Convert to TensorFlow tensors\n",
    "logits_tf = tf.convert_to_tensor(logits_np)\n",
    "true_counts_tf = tf.convert_to_tensor(true_counts_np)\n",
    "\n",
    "# Compute loss using PyTorch\n",
    "print(\"=== PyTorch Computation ===\")\n",
    "loss_fn_pt = MultinomialNLLLoss()\n",
    "loss_pt = loss_fn_pt(logits_pt, true_counts_pt).item()\n",
    "print(\"PyTorch Loss:\", loss_pt)\n",
    "\n",
    "# Compute loss using TensorFlow\n",
    "print(\"\\n=== TensorFlow Computation ===\")\n",
    "loss_tf = multinomial_nll(true_counts_tf, logits_tf).numpy()\n",
    "print(\"TensorFlow Loss:\", loss_tf)\n",
    "\n",
    "# Print difference\n",
    "print(\"\\nDifference:\", abs(loss_pt - loss_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_counts_pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_counts_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.framework.type_spec' has no attribute '_NAME_TO_TYPE_SPEC'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/wynton/home/corces/vishvak/pytorch_cbp/testing/notebooks/testing_cbp_layer_sizes.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpudev1.wynton.ucsf.edu/wynton/home/corces/vishvak/pytorch_cbp/testing/notebooks/testing_cbp_layer_sizes.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpudev1.wynton.ucsf.edu/wynton/home/corces/vishvak/pytorch_cbp/testing/notebooks/testing_cbp_layer_sizes.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtfp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpudev1.wynton.ucsf.edu/wynton/home/corces/vishvak/pytorch_cbp/testing/notebooks/testing_cbp_layer_sizes.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#from https://github.com/kundajelab/basepair/blob/cda0875571066343cdf90aed031f7c51714d991a/basepair/losses.py#L87\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpudev1.wynton.ucsf.edu/wynton/home/corces/vishvak/pytorch_cbp/testing/notebooks/testing_cbp_layer_sizes.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmultinomial_nll\u001b[39m(true_counts, logits):\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/__init__.py:23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m \u001b[39mimport\u001b[39;00m substrates\n\u001b[1;32m     21\u001b[0m \u001b[39m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m# from tensorflow_probability.google import tfp_google  # DisableOnExport\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/__init__.py:138\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m _tf_loaded():\n\u001b[1;32m    136\u001b[0m   \u001b[39m# Non-lazy load of packages that register with tensorflow or keras.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m   \u001b[39mfor\u001b[39;00m pkg_name \u001b[39min\u001b[39;00m _maybe_nonlazy_load:\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mdir\u001b[39;49m(\u001b[39mglobals\u001b[39;49m()[pkg_name])  \u001b[39m# Forces loading the package from its lazy loader.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m all_util\u001b[39m.\u001b[39mremove_undocumented(\u001b[39m__name__\u001b[39m, _lazy_load \u001b[39m+\u001b[39m _maybe_nonlazy_load)\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/internal/lazy_loader.py:57\u001b[0m, in \u001b[0;36mLazyLoader.__dir__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__dir__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 57\u001b[0m   module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load()\n\u001b[1;32m     58\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mdir\u001b[39m(module)\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/internal/lazy_loader.py:40\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_first_access \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_module_globals \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_module_globals[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_name] \u001b[39m=\u001b[39m module\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/experimental/__init__.py:31\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"TensorFlow Probability API-unstable package.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39mThis package contains potentially useful code which is under active development\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mYou are welcome to try any of this out (and tell us how well it works for you!).\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m auto_batching\n\u001b[0;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m bijectors\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m distributions\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/experimental/bijectors/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2020 The TensorFlow Probability Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# ============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"TensorFlow Probability experimental bijectors package.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbijectors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mldj_ratio\u001b[39;00m \u001b[39mimport\u001b[39;00m forward_log_det_jacobian_ratio\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbijectors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mldj_ratio\u001b[39;00m \u001b[39mimport\u001b[39;00m inverse_log_det_jacobian_ratio\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbijectors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribution_bijectors\u001b[39;00m \u001b[39mimport\u001b[39;00m make_distribution_bijector\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/bijectors/__init__.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Bijective transformations.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# pylint: disable=unused-import,wildcard-import,line-too-long,g-importing-member\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbijectors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mabsolute_value\u001b[39;00m \u001b[39mimport\u001b[39;00m AbsoluteValue\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbijectors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mascending\u001b[39;00m \u001b[39mimport\u001b[39;00m Ascending\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbijectors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatch_normalization\u001b[39;00m \u001b[39mimport\u001b[39;00m BatchNormalization\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/bijectors/absolute_value.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"AbsoluteValue bijector.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbijectors\u001b[39;00m \u001b[39mimport\u001b[39;00m bijector\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m assert_util\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m dtype_util\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/bijectors/bijector.py:34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m slicing\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m tensorshape_util\n\u001b[0;32m---> 34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m \u001b[39mimport\u001b[39;00m generic \u001b[39mas\u001b[39;00m math_generic\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m \u001b[39mimport\u001b[39;00m gradient\n\u001b[1;32m     37\u001b[0m \u001b[39m# pylint: disable=g-direct-tensorflow-import\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/math/__init__.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m all_util\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m \u001b[39mimport\u001b[39;00m ode\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m \u001b[39mimport\u001b[39;00m psd_kernels\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbessel\u001b[39;00m \u001b[39mimport\u001b[39;00m bessel_iv_ratio\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbessel\u001b[39;00m \u001b[39mimport\u001b[39;00m bessel_ive\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/math/psd_kernels/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Positive-semidefinite kernels package.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m all_util\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpsd_kernels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchangepoint\u001b[39;00m \u001b[39mimport\u001b[39;00m ChangePoint\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpsd_kernels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexp_sin_squared\u001b[39;00m \u001b[39mimport\u001b[39;00m ExpSinSquared\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpsd_kernels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexponentiated_quadratic\u001b[39;00m \u001b[39mimport\u001b[39;00m ExponentiatedQuadratic\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/math/psd_kernels/changepoint.py:28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m prefer_static \u001b[39mas\u001b[39;00m ps\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_util\n\u001b[0;32m---> 28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpsd_kernels\u001b[39;00m \u001b[39mimport\u001b[39;00m positive_semidefinite_kernel \u001b[39mas\u001b[39;00m psd_kernel\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpsd_kernels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m util\n\u001b[1;32m     31\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mChangePoint\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/math/psd_kernels/positive_semidefinite_kernel.py:1129\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(_AutoCompositeTensorPsdKernelMeta, mcs)\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(  \u001b[39m# pylint: disable=too-many-function-args\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m         mcs, classname, baseclasses, attrs)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mreturn\u001b[39;00m auto_composite_tensor\u001b[39m.\u001b[39mauto_composite_tensor(\n\u001b[1;32m   1123\u001b[0m         \u001b[39mcls\u001b[39m,\n\u001b[1;32m   1124\u001b[0m         omit_kwargs\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mparameters\u001b[39m\u001b[39m'\u001b[39m,),\n\u001b[1;32m   1125\u001b[0m         non_identifying_kwargs\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m,),\n\u001b[1;32m   1126\u001b[0m         module_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtfp.math._psdkernels\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1129\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAutoCompositeTensorPsdKernel\u001b[39;00m(PositiveSemidefiniteKernel,\n\u001b[1;32m   1130\u001b[0m                                    auto_composite_tensor\u001b[39m.\u001b[39mAutoCompositeTensor,\n\u001b[1;32m   1131\u001b[0m                                    metaclass\u001b[39m=\u001b[39m_AutoCompositeTensorPsdKernelMeta):\n\u001b[1;32m   1132\u001b[0m   \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_flatten_summand_list\u001b[39m(kernels):\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/math/psd_kernels/positive_semidefinite_kernel.py:1122\u001b[0m, in \u001b[0;36m_AutoCompositeTensorPsdKernelMeta.__new__\u001b[0;34m(mcs, classname, baseclasses, attrs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Give subclasses their own type_spec, not an inherited one.\"\"\"\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(_AutoCompositeTensorPsdKernelMeta, mcs)\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(  \u001b[39m# pylint: disable=too-many-function-args\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     mcs, classname, baseclasses, attrs)\n\u001b[0;32m-> 1122\u001b[0m \u001b[39mreturn\u001b[39;00m auto_composite_tensor\u001b[39m.\u001b[39;49mauto_composite_tensor(\n\u001b[1;32m   1123\u001b[0m     \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m   1124\u001b[0m     omit_kwargs\u001b[39m=\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mparameters\u001b[39;49m\u001b[39m'\u001b[39;49m,),\n\u001b[1;32m   1125\u001b[0m     non_identifying_kwargs\u001b[39m=\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m'\u001b[39;49m,),\n\u001b[1;32m   1126\u001b[0m     module_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtfp.math._psdkernels\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/internal/auto_composite_tensor.py:598\u001b[0m, in \u001b[0;36mauto_composite_tensor\u001b[0;34m(cls, omit_kwargs, non_identifying_kwargs, module_name)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[39m# If the declared class is already a CompositeTensor subclass, we can avoid\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[39m# affecting the actual type of the returned class. Otherwise, we need to\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[39m# explicitly mix in the CT type, and hence create and return a newly\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[39m# synthesized type.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, composite_tensor\u001b[39m.\u001b[39mCompositeTensor):\n\u001b[0;32m--> 598\u001b[0m   \u001b[39m@type_spec_register\u001b[39m(type_spec_name)\n\u001b[1;32m    599\u001b[0m   \u001b[39mclass\u001b[39;00m \u001b[39m_AlreadyCTTypeSpec\u001b[39;00m(_AutoCompositeTensorTypeSpec):\n\u001b[1;32m    601\u001b[0m     \u001b[39m@property\u001b[39m\n\u001b[1;32m    602\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mvalue_type\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    603\u001b[0m       \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/chrombpnet/lib/python3.8/site-packages/tensorflow_probability/python/internal/auto_composite_tensor.py:441\u001b[0m, in \u001b[0;36mtype_spec_register\u001b[0;34m(name, allow_overwrite)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Decorator used to register a unique name for a TypeSpec subclass.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \n\u001b[1;32m    424\u001b[0m \u001b[39mUnlike TensorFlow's `type_spec.register`, this function allows a new\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39m  A class decorator that registers the decorated class with the given name.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m \u001b[39mif\u001b[39;00m allow_overwrite \u001b[39mand\u001b[39;00m name \u001b[39min\u001b[39;00m type_spec\u001b[39m.\u001b[39;49m_NAME_TO_TYPE_SPEC:\n\u001b[1;32m    442\u001b[0m   type_spec\u001b[39m.\u001b[39m_TYPE_SPEC_TO_NAME\u001b[39m.\u001b[39mpop(\n\u001b[1;32m    443\u001b[0m       type_spec\u001b[39m.\u001b[39m_NAME_TO_TYPE_SPEC\u001b[39m.\u001b[39mpop(name))\n\u001b[1;32m    444\u001b[0m \u001b[39mreturn\u001b[39;00m type_spec\u001b[39m.\u001b[39mregister(name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.framework.type_spec' has no attribute '_NAME_TO_TYPE_SPEC'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "#from https://github.com/kundajelab/basepair/blob/cda0875571066343cdf90aed031f7c51714d991a/basepair/losses.py#L87\n",
    "def multinomial_nll(true_counts, logits):\n",
    "    \"\"\"Compute the multinomial negative log-likelihood\n",
    "    Args:\n",
    "      true_counts: observed count values\n",
    "      logits: predicted logit values\n",
    "    \"\"\"\n",
    "    counts_per_example = tf.reduce_sum(true_counts, axis=-1)\n",
    "    dist = tfp.distributions.Multinomial(total_count=counts_per_example,\n",
    "                                         logits=logits)\n",
    "    return (-tf.reduce_sum(dist.log_prob(true_counts)) / \n",
    "            tf.cast(tf.shape(true_counts)[0], dtype=tf.float32))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedMultinomialNLLLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits, true_counts):\n",
    "        # Compute log probabilities for each category\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # Compute the log-likelihood manually\n",
    "        log_likelihood = torch.sum(true_counts * log_probs, dim=-1)\n",
    "        \n",
    "        # Compute total counts per example\n",
    "        total_counts = true_counts.sum(dim=-1)\n",
    "        \n",
    "        # Compute the log-gamma term for normalization (log-gamma of total_counts + 1)\n",
    "        log_gamma_term = torch.lgamma(total_counts + 1)\n",
    "        \n",
    "        # Compute log-gamma term for each count + 1\n",
    "        log_gamma_counts = torch.lgamma(true_counts + 1).sum(dim=-1)\n",
    "        \n",
    "        # Combine the terms to compute the final log-likelihood\n",
    "        log_likelihood = log_likelihood + log_gamma_term - log_gamma_counts\n",
    "        \n",
    "        # Return negative mean log-likelihood\n",
    "        return -torch.mean(log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Multinomial NLL Loss (PyTorch): 8.918990135192871\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Example inputs\n",
    "batch_size = 4\n",
    "num_categories = 3\n",
    "np.random.seed(42)\n",
    "logits_np = np.random.randn(batch_size, num_categories).astype(np.float32)\n",
    "true_counts_np = np.random.randint(1, 10, size=(batch_size, num_categories)).astype(np.float32)\n",
    "\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "logits_torch = torch.tensor(logits_np)\n",
    "true_counts_torch = torch.tensor(true_counts_np)\n",
    "\n",
    "# Compute PyTorch loss\n",
    "loss_fn_torch = ImprovedMultinomialNLLLoss()\n",
    "loss_torch = loss_fn_torch(logits_torch, true_counts_torch).item()\n",
    "\n",
    "print(\"Improved Multinomial NLL Loss (PyTorch):\", loss_torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert numpy arrays to TensorFlow tensors\n",
    "logits_tf = tf.convert_to_tensor(logits_np, dtype=tf.float32)\n",
    "true_counts_tf = tf.convert_to_tensor(true_counts_np, dtype=tf.float32)\n",
    "\n",
    "# Compute TensorFlow loss\n",
    "loss_tf = tensorflow_multinomial_nll(true_counts_tf, logits_tf).numpy()\n",
    "\n",
    "print(\"Multinomial NLL Loss (TensorFlow):\", loss_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chrombpnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
